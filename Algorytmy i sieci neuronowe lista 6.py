import numpy as np
import matplotlib.pyplot as plt


#1. Przygotowanie zbioru danych dla operatora XOR
# Dane wejÅ›ciowe (X) i oczekiwane wyniki (y) dla operatora XOR
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y = np.array([[0], [1], [1], [0]])  # WyjÅ›cia oczekiwane



"""Co robi kod:
X definiuje dane wejÅ›ciowe jako kombinacje dwÃ³ch bitÃ³w.
y to oczekiwane wyjÅ›cia dla danych wejÅ›ciowych zgodnie z operatorem XOR.
"""

#2. Inicjalizacja parametrÃ³w sieci
# Liczba neuronÃ³w
n_input = 2  # Warstwa wejÅ›ciowa
n_output = 1  # Warstwa wyjÅ›ciowa

# Inicjalizacja wag i biasÃ³w z rozkÅ‚adu normalnego
np.random.seed(42)  # Dla powtarzalnoÅ›ci wynikÃ³w
weights = np.random.randn(n_input, n_output)  # Wagi (2x1)
biases = np.random.randn(n_output)  # Bias (1)

"""Co robi kod:
weights to macierz wag dla kaÅ¼dego wejÅ›cia (2 wejÅ›cia -> 1 neuron wyjÅ›ciowy).
biases to wektor biasÃ³w, jeden dla kaÅ¼dego neuronu wyjÅ›ciowego.
"""

#2. Inicjalizacja parametrÃ³w sieci

# Liczba neuronÃ³w
n_input = 2  # Warstwa wejÅ›ciowa
n_output = 1  # Warstwa wyjÅ›ciowa

# Inicjalizacja wag i biasÃ³w z rozkÅ‚adu normalnego
np.random.seed(42)  # Dla powtarzalnoÅ›ci wynikÃ³w
weights = np.random.randn(n_input, n_output)  # Wagi (2x1)
biases = np.random.randn(n_output)  # Bias (1)
"""Co robi kod:
weights to macierz wag dla kaÅ¼dego wejÅ›cia (2 wejÅ›cia -> 1 neuron wyjÅ›ciowy).
biases to wektor biasÃ³w, jeden dla kaÅ¼dego neuronu wyjÅ›ciowego."""

#3. Implementacja funkcji aktywacji

# Funkcja sigmoidalna
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Pochodna funkcji sigmoidalnej
def sigmoid_derivative(x):
    sig = sigmoid(x)
    return sig * (1 - sig)
"""Co robi kod:
sigmoid(x) oblicza wartoÅ›Ä‡ funkcji sigmoidalnej, uÅ¼ywanej do aktywacji neuronÃ³w.
sigmoid_derivative(x) oblicza pochodnÄ… funkcji sigmoidalnej, potrzebnÄ… do propagacji wstecznej.
"""
#4. Propagacja w przÃ³d

def forward_propagation(X, weights, biases):
    z = np.dot(X, weights) + biases  # Obliczenie sumy waÅ¼onej
    y_pred = sigmoid(z)  # Zastosowanie funkcji aktywacji
    return y_pred, z
"""Co robi kod:
z to suma waÅ¼ona obliczona jako z=ğ‘‹â‹…ğ‘¤+b
y_pred to wyjÅ›cie po zastosowaniu funkcji aktywacji."""

#5. Propagacja wstecz

def backward_propagation(X, y, y_pred, z):
    error = y_pred - y  # Obliczenie bÅ‚Ä™du
    d_weights = np.dot(X.T, error * sigmoid_derivative(z))  # Gradient wag
    d_biases = np.sum(error * sigmoid_derivative(z), axis=0)  # Gradient biasÃ³w
    return d_weights, d_biases
"""Co robi kod:
error to rÃ³Å¼nica miÄ™dzy przewidywanymi a rzeczywistymi wynikami.
d_weights i d_biases to gradienty potrzebne do aktualizacji wag i biasÃ³w."""

#6. Aktualizacja wag i biasÃ³w

def update_parameters(weights, biases, d_weights, d_biases, learning_rate):
    weights -= learning_rate * d_weights  # Aktualizacja wag
    biases -= learning_rate * d_biases  # Aktualizacja biasÃ³w
    return weights, biases
"""Co robi kod:
UÅ¼ywa gradientÃ³w i wspÃ³Å‚czynnika uczenia (learning_rate) do modyfikacji wag i biasÃ³w."""
#7. PÄ™tla uczenia

# Parametry algorytmu
learning_rate = 0.1
n_epochs = 10000  # Liczba epok

mse_history = []

for epoch in range(n_epochs):
    # Propagacja w przÃ³d
    y_pred, z = forward_propagation(X, weights, biases)
    
    # Obliczanie bÅ‚Ä™du MSE
    mse = np.mean((y - y_pred) ** 2)
    mse_history.append(mse)
    
    # Propagacja wstecz
    d_weights, d_biases = backward_propagation(X, y, y_pred, z)
    
    # Aktualizacja parametrÃ³w
    weights, biases = update_parameters(weights, biases, d_weights, d_biases, learning_rate)
    
    # Monitorowanie postÄ™pu
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, MSE: {mse:.4f}")
"""Co robi kod:
W kaÅ¼dej epoce:
Oblicza propagacjÄ™ w przÃ³d.
Liczy Å›redniokwadratowy bÅ‚Ä…d (MSE).
Oblicza gradienty w propagacji wstecz.
Aktualizuje wagi i biasy.
Monitoruje bÅ‚Ä…d co 1000 epok."""

#8. Wizualizacja wynikÃ³w


# Wykres MSE w czasie
plt.plot(mse_history)
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error (MSE)')
plt.title('MSE During Training')
plt.show()

# Testowanie sieci
y_pred, _ = forward_propagation(X, weights, biases)
print("Przewidywane wyjÅ›cia:", np.round(y_pred).flatten())
print("Oczekiwane wyjÅ›cia:", y.flatten())
"""Co robi kod:
Rysuje wykres MSE w trakcie uczenia.
Pokazuje przewidywane wyniki sieci w porÃ³wnaniu z oczekiwanymi.
Wyniki i wnioski
DziaÅ‚anie sieci: SieÄ‡ powinna poprawnie nauczyÄ‡ siÄ™ operatora XOR, osiÄ…gajÄ…c niski bÅ‚Ä…d MSE i poprawne przewidywania.
WspÃ³Å‚czynnik uczenia: Dostosowanie learning_rate wpÅ‚ywa na szybkoÅ›Ä‡ uczenia.
Liczba epok: WiÄ™cej epok pozwala na dokÅ‚adniejsze dopasowanie"""